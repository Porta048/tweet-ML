# ğŸ¤– Tweet Sentiment Analysis - AI Powered

> **Analyze the sentiment of tweets and text with state-of-the-art artificial intelligence**

A complete sentiment analysis system that uses advanced machine learning to determine if a tweet or text is **positive** or **negative**. Built with deep neural networks and featuring both a command-line interface and a beautiful web application.

## âœ¨ What does this do?

This system can read any text (like tweets, reviews, comments) and tell you if the sentiment is:
- **Positive** (happy, enthusiastic, supportive)
- **Negative** (sad, angry, disappointed)

**Example:**
- *"I love this beautiful sunny day!"* â†’ **Positive** (95% confidence)
- *"This is the worst day ever"* â†’ **Negative** (89% confidence)

## ğŸ† Performance

Our AI model achieves **enterprise-level accuracy**:
- **97.82% Accuracy** on test data
- **97.82% F1-Score** (perfect balance)
- **99.74% AUC-ROC** (near-perfect discrimination)
- **Lightning fast**: Analyzes text in milliseconds

## ğŸ¯ Key Features

### ğŸŒ **Web Application**
- **Beautiful, modern interface** with professional design
- **Real-time analysis** as you type
- **Confidence scores** and detailed breakdowns
- **Example texts** to try immediately
- **Mobile-friendly** responsive design

### ğŸ§  **Advanced AI Technology**
- **Hybrid LSTM + Transformer** architecture
- **Multi-Head Self-Attention** mechanism
- **Bidirectional processing** for context understanding
- **Smart preprocessing** with negation handling
- **Robust edge case management**

### âš¡ **Smart System Features**
- **Auto memory optimization** based on your hardware
- **GPU acceleration** when available
- **Batch processing** for large datasets
- **CSV file analysis** for business use
- **Comprehensive error handling**

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd tweet-ML

# Install dependencies
pip install -r requirements.txt
```

### 2. Launch the Web App (Easiest Way!)

```bash
python app.py
```

Then open your browser and go to: **http://localhost:5000**

That's it! ğŸ‰ You can now analyze text sentiment through the beautiful web interface.

### 3. Command Line Usage

#### Analyze a single text:
```bash
python test_model.py
```
Choose option 2 (Interactive mode) and type your text.

#### Analyze a CSV file:
```bash
python test_model.py
```
Choose option 3 and provide your CSV file path.

## ğŸ“± Web Application Usage

### Step 1: Write Your Text
Type or paste any text in the textarea (up to 280 characters).

### Step 2: Get Results
The AI will analyze your text and show:
- **Sentiment**: Positive or Negative
- **Confidence**: How sure the AI is (0-100%)
- **Detailed breakdown**: Positive % vs Negative %

### Step 3: Try Examples
Click the example buttons to see how different texts are analyzed:
- **Positive Example**: Happy, enthusiastic text
- **Negative Example**: Disappointed, sad text  
- **Neutral Example**: Factual, neutral text

## ğŸ”§ Advanced Usage

### Training Your Own Model

```bash
# Quick training with default settings
python train_sentiment_model.py

# Training with custom configuration
python run_hyperparameter_tuning.py --config config.yaml --mode training
```

### Hyperparameter Tuning

```bash
# Find the best model automatically
python run_hyperparameter_tuning.py --config config.yaml --mode tuning --method optuna --trials 50
```

### Batch Analysis

```bash
# Analyze multiple texts from CSV
python test_model.py
# Choose option 3 and provide CSV file path
```

## ğŸ“Š Technical Architecture

```
Input Text â†’ Smart Preprocessing â†’ Neural Network â†’ Sentiment Analysis
     â†“              â†“                    â†“                â†“
"I love this"  â†’ Clean & tokenize  â†’ LSTM+Attention  â†’ Positive (95%)
```

### ğŸ§© Model Components

1. **Text Preprocessing**
   - URL removal and cleaning
   - Negation handling (*"not bad"* â†’ understood correctly)
   - Special character processing
   - Smart tokenization

2. **Neural Network**
   - **Bidirectional LSTM**: Reads text forwards and backwards
   - **Multi-Head Attention**: Focuses on important words
   - **Residual Connections**: Prevents training issues
   - **Advanced Pooling**: Combines information intelligently

3. **Output Processing**
   - Confidence scoring
   - Probability distribution
   - Error handling and validation

## ğŸ“ Project Structure

```
tweet-ML/
â”œâ”€â”€ ğŸŒ app.py                    # Web application (main entry point)
â”œâ”€â”€ ğŸ“‹ templates/index.html      # Beautiful web interface
â”œâ”€â”€ ğŸ§  sentiment_model.py       # AI model architecture
â”œâ”€â”€ ğŸ¯ train_sentiment_model.py # Training system
â”œâ”€â”€ ğŸ§ª test_model.py            # Testing and analysis tools
â”œâ”€â”€ ğŸ”§ data_preparation.py      # Data processing
â”œâ”€â”€ âš™ï¸ config.yaml              # Configuration settings
â”œâ”€â”€ ğŸ“Š models/                  # Trained AI models
â”œâ”€â”€ ğŸ“ˆ data/                    # Training datasets
â””â”€â”€ ğŸ“„ README.md               # This file
```

## ğŸ›ï¸ Configuration

All settings are in `config.yaml`. Key options:

```yaml
# Model settings
model:
  embedding_dim: 128      # Word representation size
  hidden_dim: 64         # Network capacity
  use_attention: true    # Enable attention mechanism

# Training settings  
training:
  batch_size: 32         # Processing batch size
  num_epochs: 50         # Training iterations
  learning_rate: 0.001   # Learning speed

# Data settings
data:
  max_samples: 20000     # Dataset size limit
  max_length: 100        # Maximum text length
```

## ğŸ”§ Troubleshooting

### ğŸ’¾ Memory Issues
The system auto-optimizes for your hardware, but if you have problems:

```yaml
# In config.yaml, reduce these values:
training:
  batch_size: 16        # Reduce from 32
data:
  max_samples: 10000    # Reduce from 20000
model:
  hidden_dim: 32        # Reduce from 64
```

### ğŸŒ Web App Issues

```bash
# If port 5000 is busy, try:
python app.py --port 8080

# If model files are missing:
python train_sentiment_model.py  # Train a new model first
```

### ğŸ“Š Poor Performance
1. **Train with more data**: Increase `data.max_samples`
2. **Train longer**: Increase `training.num_epochs`
3. **Tune hyperparameters**: Use the tuning system

## ğŸ“ˆ Performance Details

### Metrics Achieved
- **Test Accuracy**: 97.82%
- **Precision**: 98.87% (Negative), 96.81% (Positive)  
- **Recall**: 96.74% (Negative), 98.90% (Positive)
- **F1-Score**: 97.82% (macro average)
- **AUC-ROC**: 99.74%
- **AUC-PR**: 99.79%

### What This Means
- **97.82% Accuracy**: Out of 100 texts, ~98 are classified correctly
- **High Precision**: Very few false positives
- **High Recall**: Catches almost all positive/negative cases
- **Balanced Performance**: Works equally well for both sentiments

## ğŸ¯ Use Cases

### ğŸ¢ Business Applications
- **Social media monitoring**: Track brand sentiment
- **Customer feedback analysis**: Analyze reviews automatically
- **Market research**: Understand public opinion
- **Content moderation**: Flag negative content

### ğŸ‘¨â€ğŸ’» Personal Use
- **Social media posts**: Check your tweet sentiment before posting
- **Email tone checking**: Ensure professional communication
- **Text analysis**: Understand emotional content of any text
- **Learning AI**: Educational tool for machine learning

## ğŸ¤ Contributing

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

## ğŸ“œ License

This project is licensed under the **MIT License** - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with **PyTorch** and **Flask**
- Uses **HuggingFace Datasets** for training data
- Inspired by modern NLP research and best practices
- Designed for both beginners and experts

---

## ğŸ‰ Ready to get started?

1. **Install dependencies**: `pip install -r requirements.txt`
2. **Launch web app**: `python app.py`  
3. **Open browser**: Go to `http://localhost:5000`
4. **Start analyzing!** ğŸš€

**Need help?** Open an issue or check the troubleshooting section above.

**Happy sentiment analyzing!** 

![Sentiment Analysis Demo](risultato giusto.png)
*Example: Successfully analyzing complex text with negations and preferences*

## ğŸ“Š Model Performance

| Metric | Score |
|--------|-------|
| **Validation Accuracy** | 97.93% |
| **F1-Score** | 97.93% |
| **AUC-ROC** | 99.55% |
| **Architecture** | LSTM + Transformer |
| **Vocabulary Size** | 8,826 words |
| **Training Epochs** | 67 (Early Stopped) |

## ğŸ¯ Advanced Capabilities

### Negation Handling
- âœ… "I **don't** like this" â†’ Negative
- âœ… "This is **not** good" â†’ Negative  
- âœ… "**Didn't** enjoy the experience" â†’ Negative

### Complex Linguistic Patterns
- âœ… "I learned SASS, but I **don't** like it at all, because I prefer CSS" â†’ Positive
- âœ… "Good delivery but **terrible** product" â†’ Negative
- âœ… "**Really** amazing quality" â†’ Positive

### Intensifier Recognition
- âœ… **Really**, **very**, **extremely** â†’ Properly weighted
- âœ… **Quite**, **somewhat** â†’ Moderate emphasis
- âœ… **Slightly**, **barely** â†’ Reduced emphasis

## ğŸ› ï¸ Advanced Usage (CLI)

### Train Custom Model
```bash
python train_sentiment_model.py
```

### Test Model Performance
```bash
python test_model.py
```

### Debug Preprocessing
```bash
python debug_preprocessing.py
```

### Hyperparameter Optimization
```bash
python run_hyperparameter_tuning.py --mode tuning --method optuna --trials 100
```

## ğŸ—ï¸ Technical Architecture

### Model Components
- **ğŸ“ Embedding Layer**: 128-dimensional word representations
- **ğŸ”„ Bidirectional LSTM**: 2-layer, 64 hidden units each
- **ğŸ¯ Self-Attention Mechanism**: Transformer-style attention
- **ğŸ”— Residual Connections**: Skip connections for deep learning
- **ğŸ“Š Multi-Pooling**: Max, Mean, Last, and Attention pooling
- **ğŸ§  Dense Classifier**: Multi-layer with batch normalization

### Data Pipeline
- **ğŸ“š Smart Vocabulary**: 8,826 curated tokens including special negation markers
- **ğŸ”§ Advanced Preprocessing**: Handles contractions, negations, intensifiers
- **ğŸ“ Intelligent Truncation**: Head+Tail strategy for long texts (up to 512 tokens)
- **âš–ï¸ Balanced Training**: Automatic class balancing and data augmentation

### Performance Optimizations
- **ğŸš€ Gradient Accumulation**: Efficient memory usage for large batches
- **ğŸ“‰ Learning Rate Scheduling**: Adaptive learning rate with early stopping
- **ğŸ’¾ Smart Checkpointing**: Automatic best model saving
- **ğŸ”„ Memory Management**: Automated cleanup and monitoring

## ğŸ“ Project Structure

```
tweet-ML/
â”œâ”€â”€ ğŸ“± app.py                     # Flask web application
â”œâ”€â”€ ğŸ§  sentiment_model.py         # Core ML architecture
â”œâ”€â”€ ğŸ”§ train_sentiment_model.py   # Training pipeline
â”œâ”€â”€ ğŸ§ª test_model.py             # Model evaluation
â”œâ”€â”€ ğŸ“ data_preparation.py        # Data preprocessing
â”œâ”€â”€ âš™ï¸ config_manager.py         # Configuration management
â”œâ”€â”€ ğŸ›ï¸ config.yaml              # Model configuration
â”œâ”€â”€ ğŸ“Š data/                     # Training datasets
â”œâ”€â”€ ğŸ† models/                   # Trained models and vocabularies
â”œâ”€â”€ ğŸŒ templates/               # Web interface templates
â””â”€â”€ ğŸ“‹ requirements.txt         # Dependencies
```

## ğŸ› Troubleshooting

### Common Issues

**âŒ "Vocabulary not found" error**
```bash
# Regenerate vocabulary
rm models/vocabulary.pkl
python train_sentiment_model.py
```

**âŒ Memory errors during training**
```bash
# Reduce batch size in config.yaml
training:
  batch_size: 16  # or lower
```

**âŒ Web app not loading**
```bash
# Check if model files exist
ls models/best_model.pth
ls models/vocabulary.pkl

# If missing, train the model first
python train_sentiment_model.py
```

**âŒ Unicode encoding errors (Windows)**
```bash
# Set environment variable
set PYTHONIOENCODING=utf-8
python app.py
```

## ğŸ”§ Configuration

The model is highly configurable through `config.yaml`:

```yaml
# Data settings
data:
  max_length: 512           # Maximum text length
  truncation_strategy: "head_tail"  # Smart truncation
  
# Model architecture  
model:
  embedding_dim: 256        # Embedding dimensions
  hidden_dim: 128          # LSTM hidden size
  use_self_attention: true # Transformer features
  use_residual: true       # Skip connections
  
# Training parameters
training:
  batch_size: 16           # Batch size
  learning_rate: 0.001     # Learning rate
  gradient_accumulation_steps: 4  # Memory optimization
```

## ğŸ“ Educational Value

This project demonstrates advanced NLP concepts:

- **ğŸ”¬ Hybrid Architectures**: Combining RNNs and Transformers
- **ğŸ¯ Attention Mechanisms**: Self-attention and cross-attention
- **ğŸ“Š Advanced Training**: Early stopping, learning rate scheduling
- **ğŸ› ï¸ Production Deployment**: Flask web app with error handling
- **ğŸ§ª Research Methods**: Hyperparameter tuning, ablation studies

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

---

**Built with â¤ï¸ using PyTorch, Flask, and advanced NLP techniques** 